
#解析库
1.beautifulSoup
2. PyQuery   实现jquery
3.xPath

#解决js渲染问题
1. 分析ajax
2.webdriver 一个自动化测试工具，模拟浏览器请求
3.splash 模拟浏览器库
4.PyV8、Ghost.py

#查看关系数据库工具dataGrip
robomongo

#python 库
1. urllib  py内置http请求库
2. requests
http://httpbin.org/  可用于http请求验证
代理（这个可以重看 27m左右）

正则 tool.oschina.net/regex

selenium 模拟加载 js 渲染问题


#mysql
python基础
多线程
urllib.parse  urlencode(data)  把字段转成get 参数
json.load()

链接mongodb 【抓头条】27m


#python 全局安装
1. flask 比较轻量的
2. scrapy 重点
3.pySpider

#爬虫框架


scrapy 安装（https://www.jianshu.com/p/14950bfd271b）


命令
scrapy startproject 项目名
cd 项目名
scrapy genspider quotes quotes.toscrape.com
scrapy crawl quotes

scrapy crawl  -a user=user name=name  // 传参数 __init__ 获取

scrapy crawl quotes -o quotes.json
scrapy crawl quotes -o quotes.jl // jion 没有前后的中括号
scrapy crawl quotes -o quotes.csv
scrapy crawl quotes -o quotes.xml   // .pickle  .marshal
scrapy fetch --nolog www.baidu.com // 请求
scrapy view www.baidu.com // 跳出浏览器打开
scrapy shell quotes.toscrape.com // 命令行交互调试  如输入：response 和 response.css('.quotes')
scrapy parse <url>
scrapy settings -- get
scrapy runspider  <file.py> // 运行一个文件
scrapy bench // 测试爬行速度

#Anaconda 装scrapy
conda list  列表安装的库
conda install scrapy

middlewares 处理代理

result = json.loads(response.text)
result.get('key')

# 生成项目依赖包
pip3 freeze > requirements.txt
pip3 install -r requirements.txt // 引入


